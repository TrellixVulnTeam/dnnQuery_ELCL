{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing GloVe pretrained word vectors\n",
      "\t\treading 10000 lines from GloVe file\n",
      "\t\treading 20000 lines from GloVe file\n",
      "\t\treading 30000 lines from GloVe file\n",
      "\t\treading 40000 lines from GloVe file\n",
      "\t\treading 50000 lines from GloVe file\n",
      "\t\treading 60000 lines from GloVe file\n",
      "\t\treading 70000 lines from GloVe file\n",
      "\t\treading 80000 lines from GloVe file\n",
      "\t\treading 90000 lines from GloVe file\n",
      "\t\treading 100000 lines from GloVe file\n",
      "\t\treading 110000 lines from GloVe file\n",
      "\t\treading 120000 lines from GloVe file\n",
      "\t\treading 130000 lines from GloVe file\n",
      "\t\treading 140000 lines from GloVe file\n",
      "\t\treading 150000 lines from GloVe file\n",
      "\t\treading 160000 lines from GloVe file\n",
      "\t\treading 170000 lines from GloVe file\n",
      "\t\treading 180000 lines from GloVe file\n",
      "\t\treading 190000 lines from GloVe file\n",
      "\t\treading 200000 lines from GloVe file\n",
      "\t\treading 210000 lines from GloVe file\n",
      "\t\treading 220000 lines from GloVe file\n",
      "\t\treading 230000 lines from GloVe file\n",
      "\t\treading 240000 lines from GloVe file\n",
      "\t\treading 250000 lines from GloVe file\n",
      "\t\treading 260000 lines from GloVe file\n",
      "\t\treading 270000 lines from GloVe file\n",
      "\t\treading 280000 lines from GloVe file\n",
      "\t\treading 290000 lines from GloVe file\n",
      "\t\treading 300000 lines from GloVe file\n",
      "\t\treading 310000 lines from GloVe file\n",
      "\t\treading 320000 lines from GloVe file\n",
      "\t\treading 330000 lines from GloVe file\n",
      "\t\treading 340000 lines from GloVe file\n",
      "\t\treading 350000 lines from GloVe file\n",
      "\t\treading 360000 lines from GloVe file\n",
      "\t\treading 370000 lines from GloVe file\n",
      "\t\treading 380000 lines from GloVe file\n",
      "\t\treading 390000 lines from GloVe file\n",
      "\t\treading 400000 lines from GloVe file\n",
      "Replacing GloVe word vectors as initialization\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import editdistance as ed\n",
    "import numpy as np\n",
    "import scratch\n",
    "\n",
    "vocabulary_path = './data6/vocab10000.from'\n",
    "glove_path = './glove.6B/glove.6B.100d.txt'\n",
    "max_vocabulary_size = 500000\n",
    "\n",
    "embedding_matrix, vocab, word_vector = scratch.generateEmbedMatrix(vocabulary_path, glove_path, max_vocabulary_size)\n",
    "# field2word\n",
    "# field2vec = {\"Nation\": [vec1, vec2],...}    # vec are centroids; \n",
    "                 # vec could be weighted by words frequency used for future work\n",
    "# construct field names with corresponding values and query words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_dim = 100\n",
    "def fromFieldtoWord():\n",
    "    field2word = {\n",
    "\t\t\t'sum': {'value_type': 'string', \n",
    "                       'value_range': ['combined'], \n",
    "                       'query_word': ['sum']},\n",
    "\t\t\t'diff': {'value_type': 'string', \n",
    "                       'value_range': [], \n",
    "                       'query_word': ['difference']},\n",
    "            'less': {'value_type': 'string', \n",
    "                       'value_range': [], \n",
    "                       'query_word': ['less']},\n",
    "\t\t\t'greater': {'value_type': 'string', \n",
    "                       'value_range': ['larger'], \n",
    "                       'query_word': ['more']},\n",
    "\t\t\t'mean': {'value_type': 'string', \n",
    "                       'value_range': ['average'], \n",
    "                       'query_word': []},#'mean'\n",
    "\t\t\t'argmax': {'value_type': 'string', \n",
    "                       'value_range': ['maximum','max'],# ,'last', 'previous','before'\n",
    "                       'query_word': ['most','greatest']}, #,'greatest'\n",
    "\t\t\t'argmin': {'value_type': 'string', \n",
    "                       'value_range': ['minimum'], #,'first', 'next','after'\n",
    "                       'query_word': ['least']},\n",
    "\t\t\t'Masters': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['masters']},\n",
    "\t\t\t'Country':{'value_type': 'string', \n",
    "                     'value_range': ['Brazil', 'Canada', 'Italy', 'France','Nigeria', 'Norway', 'Argentina', \n",
    "                     \t\t\t\t'South_Korea', 'Israel', 'Australia', 'Iceland', 'Slovenia', 'China', \n",
    "                        \t\t\t'Belgium', 'Germany', 'Poland', 'Spain', 'Ukraine', 'Hungary','Finland', 'Sweden', \n",
    "                        \t\t\t'Vietnam', 'Thailand', 'Switzerland', 'Russia', 'Mexico', 'Egypt', 'Singapore',\n",
    "                        \t\t\t'India', 'US', 'Czech', 'Austria', 'UK','Greece', 'Japan'], \n",
    "                     'query_word': ['country','countries']},\n",
    "            'U.S._Open': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['us','open']},\n",
    "\t\t\t'The_Open': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['open']},\n",
    "\t\t\t'PGA': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['pga']},\n",
    "\t\t\t'Team':{'value_type': 'string', \n",
    "                     'value_range': ['Brazil', 'Canada', 'Italy', 'France','Nigeria', 'Norway', 'Argentina', \n",
    "                     \t\t\t\t'South_Korea', 'Israel', 'Australia', 'Iceland', 'Slovenia', 'China', \n",
    "                        \t\t\t'Belgium', 'Germany', 'Poland', 'Spain', 'Ukraine', 'Hungary','Finland', 'Sweden', \n",
    "                        \t\t\t'Vietnam', 'Thailand', 'Switzerland', 'Russia', 'Mexico', 'Egypt', 'Singapore',\n",
    "                        \t\t\t'India', 'US', 'Czech', 'Austria', 'UK','Greece', 'Japan'], \n",
    "                     'query_word': ['team','nation']},\n",
    "            'County':{'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['county']},\n",
    "            'Years': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['years']},\n",
    "\t\t\t'Wins': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['wins']},\n",
    "\t\t\t'Areas': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['area','areas']},\n",
    "\t\t\t'Prices': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['prices', 'price']},\n",
    "            'Swara': {'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['swara']},\n",
    "            'Short_name': {'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['short','name']},\n",
    "            'Notation': {'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['notation']},\n",
    "            'Mnemonic': {'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['mnemonic']},\n",
    "\t\t\t'Player': {'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['player']},\n",
    "\t\t\t'Matches': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['matches','match']},\n",
    "\t\t\t'Innings': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['innings']},\n",
    "\t\t\t'Runs': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['runs','run']},\n",
    "\t\t\t'Average': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['average']},\n",
    "\t\t\t'100s': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['100s']},\n",
    "\t\t\t'50s': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['50s']},\n",
    "\t\t\t'Games_Played': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['games']},\n",
    "\t\t\t'Field_Goals': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['field', 'goals']},\n",
    "\t\t\t'Free_Throws': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['free', 'throws']},\n",
    "\t\t\t'Points': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['points']},\n",
    "\t\t\t'Menteri_Besar': {'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['menteri','besar']},\n",
    "\t\t\t'Party':{'value_type': 'string', \n",
    "                     'value_range': ['Conservatives', 'Green', 'Independent', 'Labour'], \n",
    "                     'query_word': ['party']},\n",
    "\t\t\t'Took_office':{'value_type': 'string', \n",
    "                     'value_range': ['Conservatives', 'Green', 'Independent', 'Labour'], \n",
    "                     'query_word': ['take', 'took']},\n",
    "            'Left_office':{'value_type': 'string', \n",
    "                     'value_range': [], \n",
    "                     'query_word': ['leave', 'left']},\n",
    "\t\t\t'Nation':{'value_type': 'string', \n",
    "                     'value_range': ['Brazil', 'Canada', 'Italy', 'France','Nigeria', 'Norway', 'Argentina', \n",
    "                     \t\t\t\t'South_Korea', 'Israel', 'Australia', 'Iceland', 'Slovenia', 'China', \n",
    "                        \t\t\t'Belgium', 'Germany', 'Poland', 'Spain', 'Ukraine', 'Hungary','Finland', 'Sweden', \n",
    "                        \t\t\t'Vietnam', 'Thailand', 'Switzerland', 'Russia', 'Mexico', 'Egypt', 'Singapore',\n",
    "                        \t\t\t'India', 'US', 'Czech', 'Austria', 'UK','Greece', 'Japan'], \n",
    "                     'query_word': ['country','nation']},\n",
    "\t\t\t'Rank': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['rank', 'ranked']},\n",
    "\t\t\t'Gold': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['gold']},\n",
    "\t\t\t'Silver': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['silver']},\n",
    "\t\t\t'Bronze': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['bronze']},\n",
    "\t\t\t'Total': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['total']},\n",
    "\t\t\t'Name': {'value_type':'string',\n",
    "\t\t\t\t\t\t'value_range':[],\n",
    "\t\t\t\t\t\t'query_word': ['name', 'who']},\n",
    "\t\t\t'Position': {'value_type':'string',\n",
    "\t\t\t\t\t\t'value_range':['goalkeeper','defender','midfielder'],\n",
    "\t\t\t\t\t\t'query_word': ['position']},\n",
    "\t\t\t'Year_inducted': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['year']},\n",
    "\t\t\t'Apps': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['appearance','appearances']},\n",
    "\t\t\t'Goals': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['goal','goals']},\n",
    "\t\t\t'Total_Apps': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['appearance','appearances']},\n",
    "\t\t\t'Total_Goals': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['goal','goals']},\n",
    "\t\t\t'League_Apps': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['appearance','appearances']},\n",
    "\t\t\t'League_Goals': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['goal','goals']},\n",
    "\t\t\t'FA_Cup_Apps': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['appearance','appearances']},\n",
    "\t\t\t'FA_Cup_Goals': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['goal','goals']},\n",
    "\t\t\t'State': {'value_type':'string',\n",
    "\t\t\t\t\t\t'value_range':['California','Texas','Florida','Louisiana'],\n",
    "\t\t\t\t\t\t'query_word': ['state']},\n",
    "\t\t\t'No._of_elected': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['elected']},\n",
    "\t\t\t'No._of_candidates': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['candidate', 'candidates']},\n",
    "\t\t\t'Total_no._of_seats_in_Assembly': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['seat', 'seats']},\n",
    "\t\t\t'Year_of_Election': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['year']},\n",
    "\t\t\t'Year': {'value_type':'int',\n",
    "\t\t\t\t\t\t'query_word': ['year']},\n",
    "\t\t\t'1st_Venue': {'value_type': 'string', \n",
    "                       'value_range': ['Chicago','Dallas', 'Florence', 'London', \n",
    "                       \t\t\t\t\t'Moscow', 'Paris', 'Philadelphia', 'Prague', 'Seattle', 'Seoul', 'Macau', 'Sydney', 'Tokyo', \n",
    "                       \t\t\t\t\t'Toronto', 'Venice', 'Vienna'], \n",
    "                       'query_word': ['city', 'cities','venue']}, \n",
    "            '2nd_Venue': {'value_type': 'string', \n",
    "                       'value_range': ['Amsterdam', 'Beijing', 'Berlin','Chicago','Dallas', 'Florence', 'London', \n",
    "                       \t\t\t\t\t'Moscow', 'Paris', 'Philadelphia', 'Prague', 'Seattle', 'Seoul',\n",
    "                       \t\t\t\t\t'Toronto', 'Venice', 'Vienna'], \n",
    "                       'query_word': ['city', 'venue']}, \n",
    "            '3rd_Venue': {'value_type': 'string', \n",
    "                       'value_range': ['Amsterdam', 'Beijing', 'Berlin','Chicago','Dallas', 'Florence', 'London', \n",
    "                       \t\t\t\t\t'Prague', 'Seattle', 'Seoul', 'Macau', 'Sydney', 'Tokyo', \n",
    "                       \t\t\t\t\t'Toronto', 'Venice', 'Vienna'], \n",
    "                       'query_word': ['city', 'venue']}, \n",
    "            '4th_Venue': {'value_type': 'string', \n",
    "                       'value_range': ['Amsterdam', 'Beijing', 'Berlin','Chicago',\n",
    "                       \t\t\t\t\t'Moscow', 'Paris', 'Philadelphia', 'Prague', 'Seattle', 'Seoul', 'Macau', 'Sydney', 'Tokyo', \n",
    "                       \t\t\t\t\t'Toronto', 'Venice', 'Vienna'], \n",
    "                       'query_word': ['city', 'venue']}, \n",
    "            '5th_Venue': {'value_type': 'string', \n",
    "                       'value_range': ['Amsterdam', 'Beijing', 'Berlin','Chicago','Dallas', 'Florence', 'London', \n",
    "                       \t\t\t\t\t'Moscow', 'Paris', 'Philadelphia', 'Macau', 'Sydney', 'Tokyo', \n",
    "                       \t\t\t\t\t'Toronto', 'Venice', 'Vienna'], \n",
    "                       'query_word': ['city', 'venue']}, \n",
    "            '6th_Venue': {'value_type': 'string', \n",
    "                       'value_range': ['Amsterdam', 'Beijing', 'Berlin','Chicago','Dallas', 'Florence', 'London', \n",
    "                       \t\t\t\t\t'Moscow', 'Paris', 'Philadelphia', 'Prague', 'Seattle', 'Seoul', 'Macau', 'Sydney', 'Tokyo'], \n",
    "                       'query_word': ['city', 'venue']}, \n",
    "\t\t\t'host_city':{'value_type': 'string', \n",
    "                       'value_range': ['Amsterdam', 'Beijing', 'Berlin','Chicago','Dallas', 'Florence', 'London', \n",
    "                       \t\t\t\t\t'Moscow', 'Paris', 'Philadelphia', 'Prague', 'Seattle', 'Seoul', 'Macau', 'Sydney', 'Tokyo', \n",
    "                       \t\t\t\t\t'Toronto', 'Venice', 'Vienna'], \n",
    "                       'query_word': ['city', 'cities']}, \n",
    "          '#_participants':{'value_type': 'int', \n",
    "                            'query_word': ['participate', 'participates', 'participated', 'participant', 'participants']},\n",
    "          '#_audience':{'value_type': 'int', \n",
    "                        'query_word': ['audience']},\n",
    "          '#_medals':{'value_type': 'int', \n",
    "                      'query_word': ['medal', 'medals']},\n",
    "          'country_size':{'value_type': 'int', \n",
    "                          'query_word': ['large', 'larger', 'largest']}, \n",
    "          'country_gdp':{'value_type': 'int', \n",
    "                         'query_word': ['gdp', 'wealth', 'wealthy', 'wealthier']}, \n",
    "          'country_population':{'value_type': 'int', \n",
    "                                'query_word': ['population', 'people']}, \n",
    "          '#_duration':{'value_type': 'int', \n",
    "                        'query_word': ['long', 'longer', 'longest', 'duration', 'day', 'days']},\n",
    "          'year':{'value_type': 'int', \n",
    "                  'query_word': ['year', 'years']}, \n",
    "          'country':{'value_type': 'string', \n",
    "                     'value_range': ['Brazil', 'Canada', 'Italy', 'France','Nigeria', 'Norway', 'Argentina', \n",
    "                     \t\t\t\t'South_Korea', 'Israel', 'Australia', 'Iceland', 'Slovenia', 'China', \n",
    "                        \t\t\t'Belgium', 'Germany', 'Poland', 'Spain', 'Ukraine', 'Hungary','Finland', 'Sweden', \n",
    "                        \t\t\t'Vietnam', 'Thailand', 'Switzerland', 'Russia', 'Mexico', 'Egypt', 'Singapore',\n",
    "                        \t\t\t'India', 'US', 'Czech', 'Austria', 'UK','Greece', 'Japan'], \n",
    "                     'query_word': ['country','countries']}}\n",
    "    return field2word\n",
    "\n",
    "def fromWordtoVec(field2word):\n",
    "    ### generate the field to vector dictionary from the field to words dictionary\n",
    "    # field2vec = {\"nation\": [vec1, vec2],...}    # vec are centroids; \n",
    "                 # vec could be weighted by words frequency used for future work\n",
    "    field2vec = dict()\n",
    "    for key in field2word:\n",
    "        field2vec[key] = []\n",
    "        if field2word[key]['value_type'] == 'string':\n",
    "            value_vector = np.array([0.0 for i in range(word_dim)])\n",
    "            count = 0\n",
    "            for word in field2word[key]['value_range']:\n",
    "                if word.lower() not in word_vector:\n",
    "                    continue\n",
    "                vec = word_vector[word.lower()]\n",
    "                value_vector += vec\n",
    "                count += 1\n",
    "            if count > 0:\n",
    "                # consider it is possible that no values is in the range\n",
    "                # so no value_vector is added\n",
    "                value_vector /= count\n",
    "                field2vec[key].append(value_vector)\n",
    "        query_vector = np.array([0.0 for i in range(word_dim)])\n",
    "        count = 0\n",
    "        for word in field2word[key]['query_word']:\n",
    "            if word.lower() not in word_vector:\n",
    "                continue\n",
    "            vec = word_vector[word.lower()]\n",
    "            query_vector += vec\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            # consider it is possible that no query words\n",
    "            # so no query_vector is appended\n",
    "            query_vector /= count\n",
    "            field2vec[key].append(query_vector)\n",
    "        if len(field2vec[key]) == 0:\n",
    "            # no vectors added, then delete\n",
    "            del field2vec[key]\n",
    "    return field2vec\n",
    "\n",
    "field2word = fromFieldtoWord()\n",
    "field2vec = fromWordtoVec(field2word)\n",
    "\n",
    "#print field2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.909090909091\n",
      "0.666666666667\n",
      "0.666666666667\n"
     ]
    }
   ],
   "source": [
    "def strSimilarity(word1, word2):\n",
    "    ### Measure how similar word1 is with respect to word2\n",
    "    diff = ed.eval(word1.lower(), word2.lower())   #search\n",
    "    # lcs = LCS(word1, word2)   #search\n",
    "    length = max(len(word1), len(word2))\n",
    "    if diff >= length:\n",
    "        similarity = 0.0\n",
    "    else:\n",
    "        similarity = 1.0 * (length-diff) / length\n",
    "    return similarity\n",
    "\n",
    "def EuDisSqu(vector1, vector2):\n",
    "    ### Calculate the square of Euclidean distance between two vectors\n",
    "    dist_square = 0.0\n",
    "    for i in range(len(vector1)):\n",
    "        dist_square += (vector1[i] - vector2[i]) * (vector1[i] - vector2[i])\n",
    "    return math.sqrt(dist_square)\n",
    "\n",
    "print strSimilarity('country gdp','country_gdp')\n",
    "print strSimilarity('ranked','rank')\n",
    "print strSimilarity('countries', 'country')\n",
    "\n",
    "# print EuDisSqu(word_vector['rank'], word_vector['ranks'])\n",
    "# print EuDisSqu(word_vector['countries'], word_vector['country'])\n",
    "# print EuDisSqu(word_vector['germany'], word_vector['france'])\n",
    "# print EuDisSqu(word_vector['germany'], word_vector['country'])\n",
    "# print EuDisSqu(word_vector['germany'], word_vector['rank'])\n",
    "# print EuDisSqu(word_vector['nation'], word_vector['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.60715626228\n",
      "[-0.10935, 0.57109, 0.98214, -0.070305, 0.27466, 0.13901, -0.6725, 0.49899, 0.21726, 0.57449, -0.43821, 0.13026, 0.25537, -0.062223, -0.52117, -0.19787, 0.67209, -0.43658, -0.59177, 0.063773, 0.46073, 0.66708, 1.1919, -0.19182, -0.13415, -0.045557, 0.052066, -0.37631, 0.49671, 0.13239, 0.22187, 0.12634, 0.34833, 0.096496, 0.28304, 0.26515, 0.41935, 0.27191, -0.78991, -0.57656, -0.97647, 0.23095, 0.48374, 0.031668, 0.33351, -0.027533, 0.61034, -0.4384, 0.0063261, -0.97056, -0.71083, -0.1317, -0.047665, 1.1634, 0.035339, -2.5317, -0.21682, 0.082702, 2.1301, 0.59453, 0.1801, 0.30265, -0.12746, -0.6372, 0.68515, 0.51137, 0.19605, 0.22645, 0.84234, -0.45817, 0.30016, -0.28873, -0.64023, -0.034291, 0.66036, -0.090798, -0.32405, 0.28764, -0.9558, 0.057511, 1.1268, 0.262, -0.23766, 0.33101, -0.71519, 0.037798, -0.40321, -0.11256, -0.37912, -0.43801, -0.14248, 0.21925, -0.35928, -0.032169, -1.2293, 0.20613, -0.20616, -0.5839, 0.66622, 0.22983]\n",
      "[-0.10935, 0.57109, 0.98214, -0.070305, 0.27466, 0.13901, -0.6725, 0.49899, 0.21726, 0.57449, -0.43821, 0.13026, 0.25537, -0.062223, -0.52117, -0.19787, 0.67209, -0.43658, -0.59177, 0.063773, 0.46073, 0.66708, 1.1919, -0.19182, -0.13415, -0.045557, 0.052066, -0.37631, 0.49671, 0.13239, 0.22187, 0.12634, 0.34833, 0.096496, 0.28304, 0.26515, 0.41935, 0.27191, -0.78991, -0.57656, -0.97647, 0.23095, 0.48374, 0.031668, 0.33351, -0.027533, 0.61034, -0.4384, 0.0063261, -0.97056, -0.71083, -0.1317, -0.047665, 1.1634, 0.035339, -2.5317, -0.21682, 0.082702, 2.1301, 0.59453, 0.1801, 0.30265, -0.12746, -0.6372, 0.68515, 0.51137, 0.19605, 0.22645, 0.84234, -0.45817, 0.30016, -0.28873, -0.64023, -0.034291, 0.66036, -0.090798, -0.32405, 0.28764, -0.9558, 0.057511, 1.1268, 0.262, -0.23766, 0.33101, -0.71519, 0.037798, -0.40321, -0.11256, -0.37912, -0.43801, -0.14248, 0.21925, -0.35928, -0.032169, -1.2293, 0.20613, -0.20616, -0.5839, 0.66622, 0.22983]\n"
     ]
    }
   ],
   "source": [
    "vectorArray = [word_vector['germany'], word_vector['france'], word_vector['countries'], word_vector['country'], \\\n",
    "              word_vector['rank'], word_vector['ranks']]\n",
    "\n",
    "def spanInSpace(vectorArray):\n",
    "    ### calculate the half of maximum distance within a list of word vectors\n",
    "    dist_square = 0.0\n",
    "    for i in range(len(vectorArray)):\n",
    "        for j in range(i+1, len(vectorArray)):\n",
    "            x = EuDisSqu(vectorArray[i], vectorArray[j])\n",
    "            if dist_square < x:\n",
    "                dist_square = x\n",
    "    diameter = dist_square\n",
    "    return diameter\n",
    "\n",
    "diameter = spanInSpace(vectorArray)\n",
    "print diameter\n",
    "\n",
    "def findNearestNeighbor(word, vectorArray, diameter):\n",
    "    ### return the word vector which is the nearest neighbor to the query word\n",
    "    ### return None is word is a new vocab or the distance between word and any of the vecs is larger than diameter\n",
    "    if word in word_vector:\n",
    "        char_vec = word_vector[word]\n",
    "    else:\n",
    "        char_vec = embedding_matrix[vocab[word], :]\n",
    "    nearest = diameter\n",
    "    idx = None\n",
    "    for i in range(len(vectorArray)):\n",
    "        x = EuDisSqu(vectorArray[i], char_vec)\n",
    "        if x > diameter:\n",
    "            continue\n",
    "        if x < nearest:\n",
    "            nearest = x\n",
    "            idx = i\n",
    "    if idx is None:\n",
    "        return None\n",
    "    else:\n",
    "        #print idx\n",
    "        #print nearest\n",
    "        return vectorArray[idx]\n",
    "\n",
    "print findNearestNeighbor('england', vectorArray, diameter)\n",
    "print word_vector['country']\n",
    "\n",
    "def strIsNum(s):\n",
    "    if not isinstance(s, basestring):\n",
    "        return 0\n",
    "    if s.isdigit():\n",
    "        return True #return int(s)\n",
    "    # for float value\n",
    "    try:\n",
    "        x = float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', 'U.S._Open', 'U.S._Open']\n",
      "['<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', 'U.S._Open', 'U.S._Open']\n",
      "['<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', '<nan>', 'U.S._Open', 'U.S._Open']\n",
      "['<nan>', '<nan>', '<nan>', '<nan>', '<nan>', 'Country', '<nan>', 'Country', 'sum', '<nan>', '<nan>', '<nan>', 'U.S._Open', 'U.S._Open']\n",
      "<nan> <nan> <nan> <nan> <nan> Country <nan> Country sum <nan> <nan> <nan> U.S._Open U.S._Open\n"
     ]
    }
   ],
   "source": [
    "def sentTagging(query, schema):\n",
    "    query = query.lower()\n",
    "    words = query.split()\n",
    "    tag = [\"<nan>\" for x in words]\n",
    "    schema0 = ['sum', 'diff'] #, 'mean', 'argmax', 'argmin']\n",
    "    schema += schema0\n",
    "    if 'Average' not in schema:\n",
    "        schema.append('mean')\n",
    "    # construct schema_vec, a dict with (key = vector for centroids, value = schema field name)\n",
    "    schema_vec = dict()\n",
    "    for i in range(len(schema)):\n",
    "        vec_list = field2vec[schema[i]]\n",
    "        for j in range(len(vec_list)):\n",
    "            vec_sign = tuple(vec_list[j])\n",
    "            #vec_sign = vec_list[j]\n",
    "            schema_vec[vec_sign] = schema[i]\n",
    "    # go over words, find nearest neighbor\n",
    "    diameter = spanInSpace(schema_vec.keys())\n",
    "#     for i in range(len(words)):    \n",
    "#         if tag[i] is not \"<nan>\":\n",
    "#             continue\n",
    "#         for j in range(len(schema)):\n",
    "#             # split around '_'\n",
    "#             div_fields = schema[j].split('_')\n",
    "#             if len(div_fields) == 1:\n",
    "#                 continue\n",
    "#             for k in range(len(div_fields)):\n",
    "#                 if strSimilarity(words[i], div_fields[k].lower()) >= 0.8:\n",
    "#                     tag[i] = schema[j]\n",
    "    # 0th pass find devided words, such as country_gdp\n",
    "    for j in range(len(schema)):\n",
    "        # split around '_'\n",
    "        div_fields = schema[j].split('_')\n",
    "        k = len(div_fields)\n",
    "        if k == 1:\n",
    "            continue\n",
    "        for i in range(len(words) - k + 1):\n",
    "            if tag[i] is not \"<nan>\":\n",
    "                continue\n",
    "            check_combine = '_'.join(words[i:i+k])\n",
    "            if strSimilarity(check_combine, schema[j].lower()) >= 0.8:\n",
    "                for l in range(k):\n",
    "                    tag[i+l] = schema[j]\n",
    "    print tag\n",
    "    # 1st pass normalize all the numbers to 00/000, but label them for decoding process\n",
    "    for i in range(len(words)):\n",
    "        if tag[i] is not \"<nan>\":\n",
    "            continue\n",
    "        if strIsNum(words[i]):\n",
    "            tag[i] = '<num>'\n",
    "    print tag\n",
    "    # 2nd pass find field name according to string similarity (field with numerical values)\n",
    "    for i in range(len(words)):    \n",
    "        if tag[i] is not \"<nan>\":\n",
    "            continue\n",
    "        for j in range(len(schema)):\n",
    "            if strSimilarity(words[i], schema[j].lower()) >= 0.8:\n",
    "                tag[i] = schema[j]\n",
    "    print tag\n",
    "    # 3rd pass find values to closest field name in semantic space (name entities)\n",
    "    filter_words = ['the','a','an','for','of','in','on','and','is','are','do','does']\n",
    "    for i in range(len(words)):    \n",
    "        if words[i] in filter_words:\n",
    "            continue\n",
    "        #          find query words to closest field name in semantic space\n",
    "        if tag[i] is not \"<nan>\":\n",
    "            continue\n",
    "        # Nearest neighbor: finding closest clustroid\n",
    "        the_one = findNearestNeighbor(words[i], schema_vec.keys(), diameter/2)\n",
    "        if the_one is not None:\n",
    "            tag[i] = schema_vec[the_one]\n",
    "    print tag\n",
    "    # (4th pass NER for un-tagged name entity field string value, tag for schema[0])\n",
    "    tag_sentence = ' '.join(tag)\n",
    "    return tag_sentence\n",
    "\n",
    "# schema = [\"Country\", \"Rank\", \"Gold\", \"Silver\", \"Bronze\", \"#_participants\", \"Total\"]\n",
    "schema = ['Country', 'Masters', 'U.S._Open', 'The_Open', 'PGA', 'Total']\n",
    "# schema = ['Team', 'County', 'Wins', 'Years', 'Areas', 'Prices']\n",
    "# query = 'how many gold medals the nation ranked 14 won'\n",
    "# query = 'the nation before australia in rank'\n",
    "query = 'how many winning golfers does australia and netherlands combined have in the u.s. open'\n",
    "# query = 'what is the difference in wins won for ballyroan abbey and greystones'\n",
    "\n",
    "print sentTagging(query, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many gold medals the nation ranked 14 won most\n",
      "<nan> <nan> Gold <nan> <nan> <nan> Rank <num> <nan> argmax\n",
      "how many winning golfers does australia and netherlands combined have in the u.s. open\n",
      "<nan> <nan> <nan> <nan> <nan> <nan> <nan> <nan> sum <nan> <nan> <nan> U.S._Open U.S._Open\n",
      "what is the difference in wins won for ballyroan abbey and greystones\n",
      "<nan> <nan> <nan> diff <nan> Wins <nan> <nan> <nan> <nan> <nan> <nan>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "_WORD_SPLIT = re.compile(b\"([,!?\\\"':;)(])\")   # get rid of '.'\n",
    "\n",
    "def basic_tokenizer(sentence):\n",
    "    ### Very basic tokenizer: split the sentence into a list of tokens.\n",
    "    words = []\n",
    "    for space_separated_fragment in sentence.strip().split():\n",
    "        words.extend(_WORD_SPLIT.split(space_separated_fragment))\n",
    "    return [w for w in words if w]\n",
    "\n",
    "def sentTagging(query, schema):\n",
    "    query = query.lower()\n",
    "    words = basic_tokenizer(query)\n",
    "    tag = [\"<nan>\" for x in words]\n",
    "    schema0 = ['sum', 'diff','less','greater','argmin','argmax'] #, 'mean', 'argmax', 'argmin']\n",
    "    schema += schema0\n",
    "    if 'Average' not in schema:\n",
    "        schema.append('mean')\n",
    "    # construct schema_vec, a dict with (key = vector for centroids, value = schema field name)\n",
    "    schema_vec = dict()\n",
    "    for i in range(len(schema)):\n",
    "        vec_list = field2vec[schema[i]]\n",
    "        for j in range(len(vec_list)):\n",
    "            vec_sign = tuple(vec_list[j])\n",
    "            #vec_sign = vec_list[j]\n",
    "            schema_vec[vec_sign] = schema[i]\n",
    "    # go over words, find nearest neighbor\n",
    "    diameter = spanInSpace(schema_vec.keys())\n",
    "#     for i in range(len(words)):    \n",
    "#         if tag[i] is not \"<nan>\":\n",
    "#             continue\n",
    "#         for j in range(len(schema)):\n",
    "#             # split around '_'\n",
    "#             div_fields = schema[j].split('_')\n",
    "#             if len(div_fields) == 1:\n",
    "#                 continue\n",
    "#             for k in range(len(div_fields)):\n",
    "#                 if strSimilarity(words[i], div_fields[k].lower()) >= 0.8:\n",
    "#                     tag[i] = schema[j]\n",
    "    # 0th pass find devided words, such as country_gdp\n",
    "    for j in range(len(schema)):\n",
    "        # split around '_'\n",
    "        div_fields = schema[j].split('_')\n",
    "        k = len(div_fields)\n",
    "        if k == 1:\n",
    "            continue\n",
    "        for i in range(len(words) - k + 1):\n",
    "            if tag[i] is not \"<nan>\":\n",
    "                continue\n",
    "            check_combine = '_'.join(words[i:i+k])\n",
    "            if strSimilarity(check_combine, schema[j].lower()) >= 0.7:\n",
    "                for l in range(k):\n",
    "                    tag[i+l] = schema[j]\n",
    "    \n",
    "    filter_words = [',','the','a','an','for','of','in','on','with','than','and','is','are','do','does','has']\n",
    "    for i in range(len(words)):\n",
    "        # 0th pass eliminate non-sense words\n",
    "        if words[i] in filter_words:\n",
    "            continue\n",
    "        # 1st pass normalize all the numbers to 00/000, but label them for decoding process\n",
    "        if tag[i] is not \"<nan>\":\n",
    "            continue\n",
    "        if strIsNum(words[i]):\n",
    "            tag[i] = '<num>'\n",
    "    \n",
    "        # 2nd pass find field name according to string similarity (field with numerical values)\n",
    "        if tag[i] is not \"<nan>\":\n",
    "            continue\n",
    "        for j in range(len(schema)):\n",
    "            if strSimilarity(words[i], schema[j].lower()) >= 0.7:\n",
    "                tag[i] = schema[j]\n",
    "    \n",
    "        # 3rd pass find values to closest field name in semantic space (name entities)\n",
    "        #          find query words to closest field name in semantic space\n",
    "        if tag[i] is not \"<nan>\":\n",
    "            continue\n",
    "        # Nearest neighbor: finding closest clustroid\n",
    "        the_one = findNearestNeighbor(words[i], schema_vec.keys(), diameter/2.7)\n",
    "        if the_one is not None:\n",
    "            tag[i] = schema_vec[the_one]\n",
    "    \n",
    "        # (4th pass NER for un-tagged name entity field string value, tag for schema[0])\n",
    "    tag_sentence = ' '.join(tag)\n",
    "    return tag_sentence\n",
    "\n",
    "schema1 = [\"Country\", \"Rank\", \"Gold\", \"Silver\", \"Bronze\", \"#_participants\", \"Total\"]\n",
    "schema2 = ['Country', 'Masters', 'U.S._Open', 'The_Open', 'PGA', 'Total']\n",
    "schema3 = ['Team', 'County', 'Wins', 'Years', 'Areas', 'Prices']\n",
    "query1 = 'how many gold medals the nation ranked 14 won most'\n",
    "# query = 'the nation before australia in rank'\n",
    "query2 = 'how many winning golfers does australia and netherlands combined have in the u.s. open'\n",
    "query3 = 'what is the difference in wins won for ballyroan abbey and greystones'\n",
    "\n",
    "print query1\n",
    "print sentTagging(query1, schema1)\n",
    "print query2\n",
    "print sentTagging(query2, schema2)\n",
    "print query3\n",
    "print sentTagging(query3, schema3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f_ta = open('./data7/rand_dev.ta', 'w')\n",
    "schemas = []\n",
    "with open('./data7/rand_dev.fi') as f_fi:\n",
    "    for sent in f_fi:\n",
    "        schema = sent.split()\n",
    "        schemas.append(schema)\n",
    "with open('./data7/rand_dev.qu') as f_qu:\n",
    "    idx = 0\n",
    "    for query in f_qu:\n",
    "        tagged = sentTagging(query, schemas[idx])\n",
    "        f_ta.write(tagged + '\\n')\n",
    "        idx += 1\n",
    "f_ta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_ta = open('./data7/rand_train.ta', 'w')\n",
    "schemas = []\n",
    "with open('./data7/rand_train.fi') as f_fi:\n",
    "    for sent in f_fi:\n",
    "        schema = sent.split()\n",
    "        schemas.append(schema)\n",
    "with open('./data7/rand_train.qu') as f_qu:\n",
    "    idx = 0\n",
    "    for query in f_qu:\n",
    "        tagged = sentTagging(query, schemas[idx])\n",
    "        f_ta.write(tagged + '\\n')\n",
    "        idx += 1\n",
    "f_ta.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading data line 200\n",
      "  reading data line 400\n",
      "  reading data line 600\n",
      "  reading data line 800\n",
      "  reading data line 1000\n",
      "  reading data line 1200\n",
      "  reading data line 1400\n",
      "  reading data line 1600\n",
      "  reading data line 1800\n",
      "  reading data line 2000\n",
      "  reading data line 2200\n",
      "  reading data line 2400\n",
      "  reading data line 2600\n",
      "  reading data line 2800\n",
      "  reading data line 3000\n",
      "bucket 0: 821\n",
      "bucket 1: 667\n",
      "bucket 2: 948\n",
      "bucket 3: 544\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import data_utils\n",
    "_buckets = [(10, 8), (13, 10), (18, 13), (22, 21)]\n",
    "\n",
    "max_size = None\n",
    "data_set = [[] for _ in _buckets]\n",
    "with tf.gfile.GFile('./data6/rand_train.qu', mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile('./data6/rand_train.lo', mode=\"r\") as target_file:\n",
    "        with tf.gfile.GFile('./data6/rand_train.ta', mode=\"r\") as tag_file:\n",
    "            source, target, tag = source_file.readline(), target_file.readline(), tag_file.readline()\n",
    "            counter = 0\n",
    "            while source and target and tag and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                if counter % 200 == 0:\n",
    "                    print(\"  reading data line %d\" % counter)\n",
    "                    sys.stdout.flush()\n",
    "                source_ids = [x for x in source.split()]\n",
    "                target_ids = [x for x in target.split()]\n",
    "                tag_ids = [x for x in tag.split()]\n",
    "                target_ids.append(data_utils.EOS_ID)\n",
    "                for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, tag_ids, target_ids])\n",
    "                        break\n",
    "                source, target, tag = source_file.readline(), target_file.readline(), tag_file.readline()\n",
    "\n",
    "print \"bucket 0: %d\" % len(data_set[0])\n",
    "print \"bucket 1: %d\" % len(data_set[1])\n",
    "print \"bucket 2: %d\" % len(data_set[2])\n",
    "print \"bucket 3: %d\" % len(data_set[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
