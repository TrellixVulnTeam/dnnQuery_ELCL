{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Version 2.0\n",
    "Edit and use for generate the 2nd version grammar archive 0507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 3, total examples: 21\n",
      "length = 5, total examples: 13\n",
      "length = 6, total examples: 148\n",
      "length = 7, total examples: 78\n",
      "length = 10, total examples: 86\n",
      "length = 11, total examples: 65\n",
      "length = 13, total examples: 1\n",
      "length = 14, total examples: 1\n",
      "length = 15, total examples: 23\n",
      "length = 19, total examples: 75\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def renew19(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens[:-1]]\n",
    "    newtokens[0] = tokens[16]\n",
    "    newtokens[1:3] = ['A', 'as']\n",
    "    newtokens[3:5] = tokens[4:6]\n",
    "    newtokens[5:9] = tokens[0:4]\n",
    "    newtokens[9:12] = ['and', 'B', 'as']\n",
    "    #newtokens[12:14] = tokens[12:14]\n",
    "    newtokens[14:] = tokens[8:12]\n",
    "    \n",
    "    if newtokens[0] == 'mean':\n",
    "        newtokens[0] = 'avg'\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew15(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens[:-1]]\n",
    "    newtokens[0:3] = tokens[12:]\n",
    "    newtokens[3:7] = tokens[8:12]\n",
    "    newtokens[7] = 'as'\n",
    "    newtokens[8:10] = tokens[4:6]\n",
    "    newtokens[10:] = tokens[0:4]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew14(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens[:-1]]\n",
    "    newtokens[0:2] = tokens[12:]\n",
    "    newtokens[2:6] = tokens[8:12]\n",
    "    newtokens[6] = 'as'\n",
    "    newtokens[7:9] = tokens[4:6]\n",
    "    newtokens[9:] = tokens[0:4]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew13(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens[:-1]]\n",
    "    newtokens[0] = tokens[10]\n",
    "    newtokens[1:3] = ['A', 'as']\n",
    "    newtokens[3:6] = tokens[0:3]\n",
    "    newtokens[6:9] = ['and','B', 'as']\n",
    "    newtokens[9:] = tokens[5:8]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew11(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:3] = tokens[8:]\n",
    "    newtokens[3:7] = tokens[0:4]\n",
    "    newtokens[7] = 'and'\n",
    "    newtokens[8:] = tokens[5:8]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew10(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:2] = tokens[8:]\n",
    "    newtokens[2:6] = tokens[0:4]\n",
    "    newtokens[6] = 'and'\n",
    "    newtokens[7:] = tokens[5:8]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew7(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:3] = tokens[4:]\n",
    "    newtokens[3:] = tokens[0:4]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew6(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:2] = tokens[4:]\n",
    "    newtokens[2:] = tokens[0:4]\n",
    "    \n",
    "    if newtokens[0] == 'mean':\n",
    "        newtokens[0] = 'avg'\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew5(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0] = 'count'\n",
    "    newtokens[1:] = tokens[0:4]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "len_search = dict()\n",
    "f2 = open('./rand_test2.lo','w')\n",
    "with open('./rand_test.lo') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        length = len(tokens)\n",
    "        if length not in len_search:\n",
    "            len_search[length] = []\n",
    "        len_search[length].append(line)\n",
    "        if length == 19:\n",
    "            newline = renew19(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 15:\n",
    "            newline = renew15(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 14:\n",
    "            newline = renew14(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 13:\n",
    "            newline = renew13(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 11:\n",
    "            newline = renew11(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 10:\n",
    "            newline = renew10(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 7:\n",
    "            newline = renew7(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 6:\n",
    "            newline = renew6(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        elif length == 5:\n",
    "            newline = renew5(line)\n",
    "            f2.write(newline+'\\n')\n",
    "        else:\n",
    "            f2.write(line)\n",
    "f2.close()\n",
    "\n",
    "for key in len_search.keys():\n",
    "    value = len_search[key]\n",
    "    print 'length = %d, total examples: %d' %(key, len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar Version 3.0\n",
    "Used for generate new version of grammar (shorter version for sum, diff, avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def renew19(line):\n",
    "    ''' from 19 to 11\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    # newly changed\n",
    "    # 'sum {comp_field1} where {comp_field2} {comp} {comp_val} and where {comp_field2} {comp} {query2_comp_val} '\n",
    "    newtokens = [x for x in tokens[:11]]\n",
    "    newtokens[0] = tokens[16]\n",
    "    newtokens[1] = tokens[5]\n",
    "    newtokens[2:6] = tokens[0:4]\n",
    "    newtokens[6] = 'and'\n",
    "    newtokens[7] = 'where'\n",
    "    #newtokens[12:14] = tokens[12:14]\n",
    "    newtokens[8:11] = tokens[9:12]\n",
    "    \n",
    "    if newtokens[0] == 'mean':\n",
    "        newtokens[0] = 'avg'\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew15(line, query):\n",
    "    ''' case 1: nested type: from 15 to 11\n",
    "        case 2: before/after: from 15 to 6\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    words = query.split()\n",
    "    if words[0] == 'of':\n",
    "        # case 1\n",
    "        newtokens = [x for x in tokens[:11]]\n",
    "        newtokens[0] = 'select'\n",
    "        newtokens[1] = tokens[13]\n",
    "        newtokens[2] = tokens[12]\n",
    "        newtokens[3] = tokens[14]\n",
    "        newtokens[4:7] = tokens[8:11]\n",
    "        newtokens[7:11] = tokens[0:4]\n",
    "    else:\n",
    "        # case 2\n",
    "        newtokens = [x for x in tokens[:6]]\n",
    "        newtokens[0] = 'select'\n",
    "        newtokens[1] = tokens[13]\n",
    "        if tokens[10] == 'greater':\n",
    "            newtokens[2] = 'next'\n",
    "        else:\n",
    "            newtokens[2] = 'prev'    \n",
    "        newtokens[3:6] = tokens[1:4]\n",
    "    \n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew14(line):\n",
    "    ''' before/after type, from 14 to 6\n",
    "        New grammar: select {field1} next/prev {field2} equal {value2}\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    # newly change\n",
    "    newtokens = [x for x in tokens[:6]]\n",
    "    newtokens[0:2] = tokens[12:]\n",
    "    if tokens[10] == 'greater':\n",
    "        newtokens[2] = 'next'\n",
    "    else:\n",
    "        newtokens[2] = 'prev'    \n",
    "    newtokens[3:6] = tokens[1:4]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew13(line):\n",
    "    ''' from 13 to 7\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    # newly changed: diff {comp_field1}' arg {max_min} {arg1} 'and' arg {max_min} {arg1}\n",
    "    newtokens = [x for x in tokens[:7]]\n",
    "    newtokens[0] = tokens[10]\n",
    "    newtokens[1] = tokens[1]\n",
    "    newtokens[2] = tokens[0]\n",
    "    newtokens[3] = tokens[2]\n",
    "    newtokens[4] = 'and'\n",
    "    newtokens[5] = tokens[5]\n",
    "    newtokens[6] = tokens[7]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew11(line):\n",
    "    ''' from 11 to 12\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:3] = tokens[8:]\n",
    "    newtokens[3:7] = tokens[0:4]\n",
    "    newtokens[7] = 'and'\n",
    "    newtokens[8:] = tokens[5:8]\n",
    "    \n",
    "    # newly added\n",
    "    newtokens.insert(0, 'select')\n",
    "    temp = newtokens[1]\n",
    "    newtokens[1] = newtokens[2]\n",
    "    newtokens[2] = temp\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew10(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:2] = tokens[8:]\n",
    "    newtokens[2:6] = tokens[0:4]\n",
    "    newtokens[6] = 'and'\n",
    "    newtokens[7:] = tokens[5:8]\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew7(line):\n",
    "    ''' from 7 to 8\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:3] = tokens[4:]\n",
    "    newtokens[3:] = tokens[0:4]\n",
    "    \n",
    "    # newly added\n",
    "    newtokens.insert(0, 'select')\n",
    "    temp = newtokens[1]\n",
    "    newtokens[1] = newtokens[2]\n",
    "    newtokens[2] = temp\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew6(line):\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0:2] = tokens[4:]\n",
    "    newtokens[2:] = tokens[0:4]\n",
    "    \n",
    "    if newtokens[0] == 'mean':\n",
    "        newtokens[0] = 'avg'\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew5(line, schema):\n",
    "    ''' from 5 to 6\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens[0] = 'count'\n",
    "    newtokens[1:] = tokens[0:4]\n",
    "    \n",
    "    # newly added\n",
    "    fields = schema.split()\n",
    "    newtokens.insert(1, fields[0])\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline\n",
    "\n",
    "def renew3(line):\n",
    "    ''' from 3 to 4\n",
    "    '''\n",
    "    tokens = line.split()\n",
    "    newtokens = [x for x in tokens]\n",
    "    newtokens.insert(0, 'select')\n",
    "    temp = newtokens[1]\n",
    "    newtokens[1] = newtokens[2]\n",
    "    newtokens[2] = temp\n",
    "    newline = ' '.join(newtokens)\n",
    "    return newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 2, total examples: 1\n",
      "length = 3, total examples: 111\n",
      "length = 5, total examples: 99\n",
      "length = 6, total examples: 739\n",
      "length = 7, total examples: 446\n",
      "length = 10, total examples: 490\n",
      "length = 11, total examples: 351\n",
      "length = 13, total examples: 3\n",
      "length = 14, total examples: 6\n",
      "length = 15, total examples: 109\n",
      "length = 19, total examples: 445\n",
      "Nation Rank Gold Silver Bronze Total\n",
      "\n",
      "what is the difference between the nations with the most and least amount of bronze medals\n",
      "\n",
      "argmax Bronze Bronze as A argmin Bronze Bronze as B diff A B\n",
      "\n",
      "Year 1st_Venue 2nd_Venue 3rd_Venue 4th_Venue 5th_Venue 6th_Venue\n",
      "\n",
      "how long in years has the this world series been occurring\n",
      "\n",
      "argmax Year Year as A argmin Year Year as B diff A B\n",
      "\n",
      "Nation Rank Gold Silver Bronze Total\n",
      "\n",
      "what is the difference between the nations with the most and least amount of gold medals\n",
      "\n",
      "argmax Gold Gold as A argmin Gold Gold as B diff A B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "len_search = dict()\n",
    "qu_search = dict()\n",
    "schema_search = dict()\n",
    "with open('./rand_train.lo') as f_lo:\n",
    "    with open('../../../data/rand_train.qu') as f_qu:\n",
    "        with open('../../../data/rand_train.fi') as f_fi:\n",
    "            logic, query, schema = f_lo.readline(), f_qu.readline(), f_fi.readline()\n",
    "            while logic and query:\n",
    "                tokens = logic.split()\n",
    "                length = len(tokens)\n",
    "                if length not in len_search:\n",
    "                    len_search[length] = []\n",
    "                    schema_search[length] = []\n",
    "                    qu_search[length] = []\n",
    "                len_search[length].append(logic)\n",
    "                qu_search[length].append(query)\n",
    "                schema_search[length].append(schema)\n",
    "                logic, query, schema = f_lo.readline(), f_qu.readline(), f_fi.readline()\n",
    "for key in len_search.keys():\n",
    "    value = len_search[key]\n",
    "    print 'length = %d, total examples: %d' %(key, len(value))\n",
    "    \n",
    "count = 0\n",
    "for line in len_search[13]:\n",
    "    idx = len_search[13].index(line)\n",
    "    print schema_search[13][idx]\n",
    "    print qu_search[13][idx]\n",
    "    print line\n",
    "#     #newline = renew15(line, qu_search[15][idx])\n",
    "#     newline = renew10(line)\n",
    "#     print newline\n",
    "#     if len(newline.split()) == 11:\n",
    "#         count += 1\n",
    "# print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length = 4, total examples: 21\n",
      "length = 6, total examples: 174\n",
      "length = 7, total examples: 1\n",
      "length = 8, total examples: 78\n",
      "length = 10, total examples: 86\n",
      "length = 11, total examples: 86\n",
      "length = 12, total examples: 65\n"
     ]
    }
   ],
   "source": [
    "lo_search = dict()\n",
    "f2 = open('./rand_test3.lo','w')\n",
    "with open('./rand_test.lo') as f_lo:\n",
    "    with open('../../../data/rand_test.qu') as f_qu:\n",
    "        with open('../../../data/rand_test.fi') as f_fi:\n",
    "            line, query, schema = f_lo.readline(), f_qu.readline(), f_fi.readline()\n",
    "            while line and query and schema:\n",
    "                tokens = line.split()\n",
    "                length = len(tokens)\n",
    "#                 if length not in lo_search:\n",
    "#                     lo_search[length] = []\n",
    "#                 lo_search[length].append(line)\n",
    "                if length == 19:\n",
    "                    newline = renew19(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 15:\n",
    "                    newline = renew15(line, query)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 14:\n",
    "                    newline = renew14(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 13:\n",
    "                    newline = renew13(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 11:\n",
    "                    newline = renew11(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 10:\n",
    "                    newline = renew10(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 7:\n",
    "                    newline = renew7(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 6:\n",
    "                    newline = renew6(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 5:\n",
    "                    newline = renew5(line, schema)\n",
    "                    f2.write(newline+'\\n')\n",
    "                elif length == 3:\n",
    "                    newline = renew3(line)\n",
    "                    f2.write(newline+'\\n')\n",
    "                else:\n",
    "                    f2.write(line)\n",
    "                newlength = len(newline.split())\n",
    "                if newlength not in lo_search:\n",
    "                    lo_search[newlength] = []\n",
    "                lo_search[newlength].append(newline)\n",
    "                line, query, schema = f_lo.readline(), f_qu.readline(), f_fi.readline()\n",
    "f2.close()\n",
    "\n",
    "for key in lo_search.keys():\n",
    "    value = lo_search[key]\n",
    "    print 'length = %d, total examples: %d' %(key, len(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.parse import stanford\n",
    "from nltk import tree\n",
    "os.environ['STANFORD_PARSER'] = '/Users/richard_xiong/Documents/DeepLearningMaster/deep_parser'\n",
    "os.environ['STANFORD_MODELS'] = '/Users/richard_xiong/Documents/DeepLearningMaster/deep_parser'\n",
    "\n",
    "parser = stanford.StanfordParser(model_path='/Users/richard_xiong/Documents/DeepLearningMaster/deep_parser/englishPCFG.ser.gz')\n",
    "\n",
    "parsequery = \"when the <field> was <value> and the <field> was <value> , which <field> was the most recent <field> \"\n",
    "dependency_tree = parser.raw_parse_sents((parsequery, 'Hello, My name is Melroy'))\n",
    "\n",
    "for line in dependency_tree[0]:\n",
    "    line.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
