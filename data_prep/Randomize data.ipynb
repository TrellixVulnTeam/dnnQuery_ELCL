{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1787  876  286 ...,  952  724 3049]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import editdistance as ed\n",
    "import numpy as np\n",
    "\n",
    "Fqu = []\n",
    "Flo = []\n",
    "Ffi = []\n",
    "\n",
    "with open('../data_archive/0420/rand_train3.fi') as f_fi:\n",
    "    with open('../data_archive/0420/rand_train3.qu') as f_qu:\n",
    "        with open('../data_archive/0420/rand_train3.lo') as f_lo:\n",
    "            schema, query, logic = f_fi.readline(), f_qu.readline(), f_lo.readline()\n",
    "            idx = 0\n",
    "            while schema and query and logic:\n",
    "                Fqu.append(query)\n",
    "                Flo.append(logic)\n",
    "                Ffi.append(schema)\n",
    "                schema, query, logic = f_fi.readline(), f_qu.readline(), f_lo.readline()\n",
    "                \n",
    "num = np.random.permutation(len(Fqu))\n",
    "print num\n",
    "f_fi1 = open('../data_archive/0420/rand_train1.fi', 'w')\n",
    "f_lo1 = open('../data_archive/0420/rand_train1.lo', 'w')\n",
    "f_qu1 = open('../data_archive/0420/rand_train1.qu', 'w')\n",
    "\n",
    "for i in range(len(num)):\n",
    "    f_lo1.write(Flo[num[i]])\n",
    "    f_fi1.write(Ffi[num[i]])\n",
    "    f_qu1.write(Fqu[num[i]])\n",
    "\n",
    "f_fi1.close()\n",
    "f_qu1.close()\n",
    "f_lo1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading data line 200\n",
      "  reading data line 400\n",
      "  reading data line 600\n",
      "  reading data line 800\n",
      "  reading data line 1000\n",
      "  reading data line 1200\n",
      "  reading data line 1400\n",
      "  reading data line 1600\n",
      "  reading data line 1800\n",
      "  reading data line 2000\n",
      "  reading data line 2200\n",
      "  reading data line 2400\n",
      "  reading data line 2600\n",
      "  reading data line 2800\n",
      "bucket 0: 801\n",
      "bucket 1: 732\n",
      "bucket 2: 711\n",
      "bucket 3: 556\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_utils_tag\n",
    "\n",
    "_buckets = [(10, 8), (15, 12), (19, 16), (23, 21)]  # _buckets = [(11, 8), (15, 12), (20, 16), (24, 21)]\n",
    "max_size = None\n",
    "data_set = [[] for _ in _buckets]\n",
    "\n",
    "with tf.gfile.GFile('../data/rand_train.qu', mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile('../data/rand_train.lo', mode=\"r\") as target_file:\n",
    "        with tf.gfile.GFile('../data/rand_train.ta', mode=\"r\") as tag_file:\n",
    "            source, target, tag = source_file.readline(), target_file.readline(), tag_file.readline()\n",
    "            counter = 0\n",
    "            while source and target and tag and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                if counter % 200 == 0:\n",
    "                    print(\"  reading data line %d\" % counter)\n",
    "                    sys.stdout.flush()\n",
    "                source_ids = [x for x in source.split()]\n",
    "                target_ids = [x for x in target.split()]\n",
    "                tag_ids = [x for x in tag.split()]\n",
    "                target_ids.append(data_utils_tag.EOS_ID)\n",
    "                for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, tag_ids, target_ids])\n",
    "                        break\n",
    "                source, target, tag = source_file.readline(), target_file.readline(), tag_file.readline()\n",
    "\n",
    "print \"bucket 0: %d\" % len(data_set[0])\n",
    "print \"bucket 1: %d\" % len(data_set[1])\n",
    "print \"bucket 2: %d\" % len(data_set[2])\n",
    "print \"bucket 3: %d\" % len(data_set[3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
