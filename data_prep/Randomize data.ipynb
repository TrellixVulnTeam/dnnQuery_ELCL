{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2384 11592  4312 ...,  7189  1458  3798]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import editdistance as ed\n",
    "import numpy as np\n",
    "\n",
    "Fqu = []\n",
    "Flo = []\n",
    "Ffi = []\n",
    "\n",
    "with open('./rand1.fi') as f_fi:\n",
    "    with open('./rand1.qu') as f_qu:\n",
    "        with open('./rand1.lo') as f_lo:\n",
    "            schema, query, logic = f_fi.readline(), f_qu.readline(), f_lo.readline()\n",
    "            idx = 0\n",
    "            while schema and query and logic:\n",
    "                Fqu.append(query)\n",
    "                Flo.append(logic)\n",
    "                Ffi.append(schema)\n",
    "                schema, query, logic = f_fi.readline(), f_qu.readline(), f_lo.readline()\n",
    "                \n",
    "num = np.random.permutation(len(Fqu))\n",
    "print num\n",
    "f_fi1 = open('./rand1.fi', 'w')\n",
    "f_lo1 = open('./rand1.lo', 'w')\n",
    "f_qu1 = open('./rand1.qu', 'w')\n",
    "\n",
    "for i in range(len(num)):\n",
    "    f_lo1.write(Flo[num[i]])\n",
    "    f_fi1.write(Ffi[num[i]])\n",
    "    f_qu1.write(Fqu[num[i]])\n",
    "\n",
    "f_fi1.close()\n",
    "f_qu1.close()\n",
    "f_lo1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_fi1 = open('../data/rand.fi', 'w')\n",
    "f_lo1 = open('../data/rand.lo', 'w')\n",
    "f_qu1 = open('../data/rand.qu', 'w')\n",
    "\n",
    "for i in range(len(num)):\n",
    "    f_lo1.write(Flo[num[i]])\n",
    "    f_fi1.write(Ffi[num[i]])\n",
    "    f_qu1.write(Fqu[num[i]])\n",
    "\n",
    "f_fi1.close()\n",
    "f_qu1.close()\n",
    "f_lo1.close()\n",
    "\n",
    "f_fi_train = open('../data/rand_train.fi', 'w')\n",
    "f_lo_train = open('../data/rand_train.lo', 'w')\n",
    "f_qu_train = open('../data/rand_train.qu', 'w')\n",
    "f_fi_dev = open('../data/rand_dev.fi', 'w')\n",
    "f_lo_dev = open('../data/rand_dev.lo', 'w')\n",
    "f_qu_dev = open('../data/rand_dev.qu', 'w')\n",
    "f_fi_test = open('../data/rand_test.fi', 'w')\n",
    "f_lo_test = open('../data/rand_test.lo', 'w')\n",
    "f_qu_test = open('../data/rand_test.qu', 'w')\n",
    "\n",
    "for i in range(len(num)):\n",
    "    if i < 8000:\n",
    "        f_lo_train.write(Flo[num[i]])\n",
    "        f_fi_train.write(Ffi[num[i]])\n",
    "        f_qu_train.write(Fqu[num[i]])\n",
    "        continue\n",
    "    if i < 10000:\n",
    "        f_lo_dev.write(Flo[num[i]])\n",
    "        f_fi_dev.write(Ffi[num[i]])\n",
    "        f_qu_dev.write(Fqu[num[i]])\n",
    "        continue\n",
    "    f_lo_test.write(Flo[num[i]])\n",
    "    f_fi_test.write(Ffi[num[i]])\n",
    "    f_qu_test.write(Fqu[num[i]])\n",
    "\n",
    "\n",
    "f_fi_train.close()\n",
    "f_qu_train.close()\n",
    "f_lo_train.close()\n",
    "f_fi_dev.close()\n",
    "f_qu_dev.close()\n",
    "f_lo_dev.close()\n",
    "f_fi_test.close()\n",
    "f_qu_test.close()\n",
    "f_lo_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading data line 200\n",
      "  reading data line 400\n",
      "  reading data line 600\n",
      "  reading data line 800\n",
      "  reading data line 1000\n",
      "  reading data line 1200\n",
      "  reading data line 1400\n",
      "  reading data line 1600\n",
      "  reading data line 1800\n",
      "  reading data line 2000\n",
      "  reading data line 2200\n",
      "  reading data line 2400\n",
      "  reading data line 2600\n",
      "  reading data line 2800\n",
      "  reading data line 3000\n",
      "  reading data line 3200\n",
      "  reading data line 3400\n",
      "  reading data line 3600\n",
      "  reading data line 3800\n",
      "  reading data line 4000\n",
      "  reading data line 4200\n",
      "  reading data line 4400\n",
      "  reading data line 4600\n",
      "  reading data line 4800\n",
      "  reading data line 5000\n",
      "  reading data line 5200\n",
      "  reading data line 5400\n",
      "  reading data line 5600\n",
      "  reading data line 5800\n",
      "  reading data line 6000\n",
      "  reading data line 6200\n",
      "  reading data line 6400\n",
      "  reading data line 6600\n",
      "  reading data line 6800\n",
      "  reading data line 7000\n",
      "  reading data line 7200\n",
      "  reading data line 7400\n",
      "  reading data line 7600\n",
      "  reading data line 7800\n",
      "  reading data line 8000\n",
      "bucket 0: 2631\n",
      "bucket 1: 2740\n",
      "bucket 2: 2629\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_utils_tag\n",
    "\n",
    "#_buckets = [(10, 8), (14, 12), (18, 15), (23, 20)]  \n",
    "# _buckets = [(11, 8), (15, 12), (20, 16), (24, 21)]\n",
    "# _buckets = [(10, 8), (15, 12), (19, 16)]\n",
    "_buckets = [(11, 8), (15, 12), (19, 14)]  # new logical forms\n",
    "max_size = None\n",
    "data_set = [[] for _ in _buckets]\n",
    "stat = dict()\n",
    "\n",
    "with tf.gfile.GFile('../data/rand_train.qu', mode=\"r\") as source_file:\n",
    "    with tf.gfile.GFile('../data/rand_train.lo', mode=\"r\") as target_file:\n",
    "        with tf.gfile.GFile('../data/rand_train.ta', mode=\"r\") as tag_file:\n",
    "            source, target, tag = source_file.readline(), target_file.readline(), tag_file.readline()\n",
    "            counter = 0\n",
    "            while source and target and tag and (not max_size or counter < max_size):\n",
    "                counter += 1\n",
    "                if counter % 200 == 0:\n",
    "                    print(\"  reading data line %d\" % counter)\n",
    "                    sys.stdout.flush()\n",
    "                source_ids = [x for x in source.split()]\n",
    "                target_ids = [x for x in target.split()]\n",
    "                tag_ids = [x for x in tag.split()]\n",
    "                target_ids.append(data_utils_tag.EOS_ID)\n",
    "                if (len(source_ids), len(target_ids)) not in stat:\n",
    "                    stat[(len(source_ids), len(target_ids))] = 0\n",
    "                stat[(len(source_ids), len(target_ids))] += 1\n",
    "                for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
    "                    if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                        data_set[bucket_id].append([source_ids, tag_ids, target_ids])\n",
    "                        break\n",
    "                source, target, tag = source_file.readline(), target_file.readline(), tag_file.readline()\n",
    "\n",
    "print \"bucket 0: %d\" % len(data_set[0])\n",
    "print \"bucket 1: %d\" % len(data_set[1])\n",
    "print \"bucket 2: %d\" % len(data_set[2])\n",
    "#print \"bucket 3: %d\" % len(data_set[3])\n",
    "\n",
    "# for (s_size, t_size) in sorted(stat.keys()):\n",
    "#     print '(%d, %d): %d' %(s_size, t_size, stat[(s_size, t_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
